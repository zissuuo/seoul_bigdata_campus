####step 1. Hypothesis
#H0: m1 = m2 vs H1: m1 =/= m2
####step 2. Find a information in question
m1 <- 22.3
m2 <- 25.91
n1 <- 43
n2 <- 50
s1 <- 6.88
s2 <- 8.01
####step 3. s1 and s2 are not same, so we should use two-sided tests.
####step 4. P-value
#test statistics
SE <- sqrt(s1^2/n1 + s2^2/n2)
test.stat1 <- (m1 - m2)/SE; test.stat1
#degree of freedom
df <- n1 + n2 - 2; df
#p-value
pvalue1 <- 2 * pt(test.stat, df); pvalue1
#p.value < 0.05 → Reject Null hypothesis(H0)
#The mean of sales for two menus (A and B) has significant difference.
####step 5.CI
CL <- 0.95
margin_of_error <- qt((1 - CL) / 2, df) * SE
CI_upper <- (m2 - m1) - margin_of_error
CI_lower <- (m2 - m1) + margin_of_error
cat("95% 신뢰구간:", CI_lower, "-", CI_upper, "\n")
#95% 신뢰구간: 0.5429912 - 6.677009
#CI did not include zero.
#The mean of sales for two menus (A and B) has significant difference.
#Conclusion: Using a P-value and CI are both meaningful test and informative.
########Exercise 5.15
####step 1. Find a infromation in question.
# Heights Data
mm = 175.4
mw = 161.7
sd1 = 7
#Marathon record Data
mtm = 221
mtw = 248
sd2 = 40
#Effect Size
Esize_H = (mm - mw) / sd1 ; Esize_H
Esize_T = (mtw - mtm) / sd2; Esize_T
#Esize_H is greater than Esize_T.
########Exercise 5.18a
####step 1. Loading data
anor <- read.table("/Users/kimjisu/Desktop/교육통계적방법/Anorexia.dat", header=TRUE)
View(anor)
####step 2. Generate 'change of weight(after-before)' variables
anor$weight = anor$after - anor$before
####step 3. calculating mean, sd, n (cb = group1, f = group2)
anor_cb <- anor$weight[anor$therapy == 'cb']
anor_fam <- anor$weight[anor$therapy == 'f']
mg1 = mean(anor_cb)
mg2 = mean(anor_fam)
ng1 = 29
ng2 = 17
sg1 = sd(anor_cb)
sg2 = sd(anor_fam)
####step 4. Test stat
SE2 = sqrt(sg1^2/ng1 + sg2^2/ng2)
test.stat2 = (mg1 - mg2) / SE2; test.stat2
df_t = 29 + 17 - 2
####step 5. P-value
pvalue2 = 2 * pt(test.stat2, df_t) ; pvalue2
########Exercise 5.12
####step 1. Hypothesis
#H0: m1 = m2 vs H1: m1 =/= m2
####step 2. Find a information in question
m1 <- 22.3
m2 <- 25.91
n1 <- 43
n2 <- 50
s1 <- 6.88
s2 <- 8.01
####step 3. s1 and s2 are not same, so we should use two-sided tests.
####step 4. P-value
#test statistics
SE <- sqrt(s1^2/n1 + s2^2/n2)
test.stat1 <- (m1 - m2)/SE; test.stat1
#degree of freedom
df <- n1 + n2 - 2; df
#p-value
pvalue1 <- 2 * pt(test.stat, df); pvalue1
#p.value < 0.05 → Reject Null hypothesis(H0)
#The mean of sales for two menus (A and B) has significant difference.
####step 5.CI
CL <- 0.95
margin_of_error <- qt((1 - CL) / 2, df) * SE
CI_upper <- (m2 - m1) - margin_of_error
CI_lower <- (m2 - m1) + margin_of_error
cat("95% 신뢰구간:", CI_lower, "-", CI_upper, "\n")
#95% 신뢰구간: 0.5429912 - 6.677009
#CI did not include zero.
#The mean of sales for two menus (A and B) has significant difference.
#Conclusion: Using a P-value and CI are both meaningful test and informative.
########Exercise 5.15
####step 1. Find a infromation in question.
# Heights Data
mm = 175.4
mw = 161.7
sd1 = 7
#Marathon record Data
mtm = 221
mtw = 248
sd2 = 40
#Effect Size
Esize_H = (mm - mw) / sd1 ; Esize_H
Esize_T = (mtw - mtm) / sd2; Esize_T
#Esize_H is greater than Esize_T.
########Exercise 5.18a
####step 1. Loading data
anor <- read.table("/Users/kimjisu/Desktop/교육통계적방법/Anorexia.dat", header=TRUE)
####step 2. Generate 'change of weight(after-before)' variables
anor$weight = anor$after - anor$before
#View(anor)
####step 3. calculating mean, sd, n (cb = group1, f = group2)
anor_cb <- anor$weight[anor$therapy == 'cb']
anor_fam <- anor$weight[anor$therapy == 'f']
mg1 = mean(anor_cb)
mg2 = mean(anor_fam)
ng1 = 29
ng2 = 17
sg1 = sd(anor_cb)
sg2 = sd(anor_fam)
####step 4. Test stat
SE2 = sqrt(sg1^2/ng1 + sg2^2/ng2)
test.stat2 = (mg1 - mg2) / SE2; test.stat2
df_t = 29 + 17 - 2
####step 5. P-value
pvalue2 = 2 * pt(test.stat2, df_t) ; pvalue2
(50.8 - 43.6) / 43.6
0.108/sqrt(88) + 0.336/sqrt(109)
0.108*(1-0.108)/sqrt(88) + 0.336*(1-0.336)/sqrt(109)
SE = sqrt(13.92^2/68 + 12.272^2/108)
t.test = 6 / se
t.test = 6 / SE
13.92^2/68
12.272^2/108
SE = sqrt(13.92^2/68 + 12.272^2/108)
sqrt(4.23)
t.test = (79.92 - 73.92)/sqrt(4.23)
SE = sqrt(13.92^2/78 + 12.272^2/108)
SE = sqrt(13.92^2/78 + 12.272^2/108)
t.test = (79.92 - 73.92)/SE
13.92^2/78
SE = sqrt(13.92^2/78 + 12.272^2/108)
t.test = (79.92 - 73.92)/SE
SE = sqrt(13.92^2/78 + 12.27^2/108)
t.test = (79.92 - 73.92)/SE
SE = sqrt(13.92^2/78 + 12.272^2/108)
sqrt(88)*0.108 + sqrt(109)*0.336
SE = sqrt(13.92^2/78 + 12.272^2/108)
t.test = (79.92 - 73.92)/SE
SE = sqrt(13.92^2/78 + 12.272^2/108)
t.test = (79.92 - 73.92)/SE
SE = sqrt(10.54^2/75 + 19.414^2/105)
t.test = 6 / SE
Races <- read.table("http://stat4ds.rwth-aachen.de/data/ScotsRaces.dat", header = TRUE)
View(Races)
View(Races[-41,]$timeW)
Races[-41,]$timeW
Races[-41,]
View(Races[-41,])
Anor = read.table("http://stat4ds.rwth-aachen.de/data/Anorexia.dat", header = TRUE)
head(Anor)
y1 = Anor$after[Anor$therapy == "cb"] - Anor$after[Anor$therapy == "cb"]
y2 = Anor$after[Anor$therapy == "c"] - Anor$after[Anor$therapy == "c"]
t.anor <- t.test(y1, y2, var.equal = TRUE)
install.packages("BayesFactor")
library("BayesFactor")
install.packages("MatrixModels")
install.packages("MatrixModels")
uninstall.packages("MatrixModels")
theta <- seq(-3, 3, .1); theta
## or
assign("theta", seq(-3,3,.1)); theta
bmedium <- 0; amoderate <- 1
P <- 1 / (1 + exp(-amoderate * (theta - bmedium)))
plot(theta, P)
plot(theta, P, type="l")
plot(theta, P)
plot(theta, P, type="l")
par(lab=c(7,3,3))
theta <- seq(-3, 3, .1)
b <- 0
a <- 1.2
P <- 1 / (1 + exp(-a * (theta - b)))
a
### Plot example 1
plot(theta, P, type="l", xlim=c(-3,3), ylim=c(0,1),
xlab=expression(paste("Ability, ",theta)),
ylab=expression(paste(
"Probability of Correct Response, P(",theta,")")))
### Plot example 2
plot(theta, P, type="l", xlim=c(-3,3), ylim=c(0,1),
xlab="Ability", ylab="Probability of Correct Response",
main="Figure 1. An Item Characteristic Curve with
Medium Item Difficulty and Moderate Item Discrimination",
sub="See Baker and Kim (2017).")
plot(theta, P, type="l", xlim=c(-3,3), ylim=c(0,1),
xlab="Ability", ylab="Probability of Correct Response")
iccplot(0, 1)
iccplot(a=1, b=0)
iccplot(0, 1)
iccplot(a=1, b=0)
iccplot <- function(b, a) {
par(lab=c(7,3,3))
theta <- seq(-3, 3, .1)
P <- 1 / (1 + exp(-a * (theta - b)))
plot(theta, P, type="l", xlim=c(-3,3), ylim=c(0,1),
xlab="Ability", ylab="Probability of Correct Response")
}
iccplot(0, 1)
iccplot(a=1, b=0)
icccal <- function(b, a, c) {
if (missing(c)) c <- 0
if (missing(a)) a <- 1
theta <- seq(-3, 3, 1)
L <- a * (theta - b)
expnl <- exp(-L)
opexpnl <- 1 + expnl
P <- c + (1 - c) / opexpnl
data.frame(theta, L, expnl, opexpnl, P)
}
icccal(1)
icccal(1, 0.5)
?runif
lab=c(7,5,3)
par(lab=c(7,5,3))
plot(theta, p, xlim=c(-3,3), ylim=c(0,1),
xlab="Ability", ylab="Probability of Correct Response")
theta <- seq(-3, 3, .1875)
f <- rep(21, length(theta))
wb <- round(runif(1,-3,3), 2) #runif를 통해 랜덤추출
wa <- round(runif(1,0.2,2.8), 2)
wc <- round(runif(1,0,.35), 2)
mdl <- 2
if (mdl == 1 | mdl == 2) { wc <- 0 }
if (mdl == 1) { wa <- 1 }
for (g in 1:length(theta)) {
P <- wc + (1 - wc) / (1 + exp(-wa * (theta - wb)))
}
p <- rbinom(length(theta), f, P) / f
par(lab=c(7,5,3))
plot(theta, p, xlim=c(-3,3), ylim=c(0,1),
xlab="Ability", ylab="Probability of Correct Response")
# True Par
TrueA = c(1, 1.5, 1.7, 2)
TrueB = c(-1, 0, 1, 1.5)
TrueC = c(0.2, 0.3, 0.2, 0.3)
I = length(TrueA)
# True Theta
N = 1000
theta = rnorm(N)
ThreePL = function(theta, a, b, c, D = 1){
c + (1-c)/(1 + exp(-D*a*(theta - b)))
}
P = sapply(1:I, function(i){ThreePL(theta, a = TrueA[i], b =TrueB[i], c =  TrueC[i])})
R = matrix(runif(N*I), nrow = N, ncol = I)
resp = ifelse(P > R , 1, 0)
resp
Q = 41
qpoint = seq(from = -4, to = 4, length.out = Q) # X_k 0.2 point로 구분
weight = dnorm(qpoint)/sum(dnorm(qpoint)) # A(X_k) 정규분포 상 해당 집단에 포함될 확률
weight
qpoint
# 3 PL 초기 4개 문항 Item parameter
par.old = matrix(rep(c(1, 0, 0.2), each = I), nrow= I)
iter = 0
repeat{
iter = iter +1
print(paste0("Iteration: ", iter))
# E step: Update f and r
P_q = sapply(1:I, function(i){
ThreePL(qpoint, a = par.old[i,1], b = par.old[i,2], c =  par.old[i,3])})
table0 = exp(log(P_q)%*%t(resp) + log(1-P_q)%*%t(1-resp)) # 6.21 Q by N
table0 = table0*weight # 6.21 Q by N
demo = colSums(table0) # Q by N, 6.22 = 6.23 without summation over j
f_mat = table0/matrix(rep(demo, each = Q), nrow= Q) # Q by N, 6.22 = 6.23 without summation over j
fk = rowSums(f_mat) # 6.23
rk_mat = f_mat %*% resp # 6.23
#  sum(fk)
#  colSums(rk_mat)
# M step: Optimization
fn = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
return(-sum(rk*log(Pk) + (fk-rk)*log(1-Pk)))
} # fn funtion : function to be minimized
gr = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
Pk_star = (Pk - par[3])/(1 - par[3]) # c par
Wk =  Pk_star *(1-Pk_star)/(Pk*(1-Pk)) # c par
a.L = -(1 - par[3])*sum((qpoint - par[2])*(rk-fk*Pk)*Wk) # 6.25
b.L = par[1]*(1 - par[3])*sum((rk-fk*Pk)*Wk)             # 6.26
c.L = -1/(1-par[3])*sum((rk-fk*Pk)/Pk)                   # 6.27
return(c(a.L, b.L, c.L))} # gr function: function to return the gradient
par.new = matrix(nrow = I, ncol = 3)
i = 1
for(i in 1:I){
rk = rk_mat[,i] # E-step
res = optim(par.old[i, ], fn, gr, method = "BFGS") # M-step
par.new[i, ] = res$par
}# i loop
if(max(abs(par.old - par.new)) < 0.0001) break
par.old  = par.new
}# repeat
table0 = exp(log(P_q)%*%t(resp) + log(1-P_q)%*%t(1-resp)) # 6.21 Q by N
table0 = table0*weight # 6.21 Q by N
demo = colSums(table0) # Q by N, 6.22 = 6.23 without summation over j
f_mat = table0/matrix(rep(demo, each = Q), nrow= Q) # Q by N, 6.22 = 6.23 without summation over j
fk = rowSums(f_mat) # 6.23
rk_mat = f_mat %*% resp # 6.23
fk
rk_mat
# M step: Optimization
fn = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
return(-sum(rk*log(Pk) + (fk-rk)*log(1-Pk)))
} # fn funtion : function to be minimized
gr = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
Pk_star = (Pk - par[3])/(1 - par[3]) # c par
Wk =  Pk_star *(1-Pk_star)/(Pk*(1-Pk)) # c par
a.L = -(1 - par[3])*sum((qpoint - par[2])*(rk-fk*Pk)*Wk) # 6.25
b.L = par[1]*(1 - par[3])*sum((rk-fk*Pk)*Wk)             # 6.26
c.L = -1/(1-par[3])*sum((rk-fk*Pk)/Pk)                   # 6.27
return(c(a.L, b.L, c.L))} # gr function: function to return the gradient
par.new = matrix(nrow = I, ncol = 3)
i = 1
for(i in 1:I){
rk = rk_mat[,i] # E-step
res = optim(par.old[i, ], fn, gr, method = "BFGS") # M-step
par.new[i, ] = res$par
}# i loop
par.new[i, ]
par.new
repeat{
iter = iter +1
print(paste0("Iteration: ", iter))
# E step: Update f and r
P_q = sapply(1:I, function(i){
ThreePL(qpoint, a = par.old[i,1], b = par.old[i,2], c =  par.old[i,3])})
table0 = exp(log(P_q)%*%t(resp) + log(1-P_q)%*%t(1-resp)) # 6.21 Q by N
table0 = table0*weight # 6.21 Q by N
demo = colSums(table0) # Q by N, 6.22 = 6.23 without summation over j
f_mat = table0/matrix(rep(demo, each = Q), nrow= Q) # Q by N, 6.22 = 6.23 without summation over j
fk = rowSums(f_mat) # 6.23
rk_mat = f_mat %*% resp # 6.23
# 각  문항에 대해서 맞춘 개 수
#  sum(fk)
#  colSums(rk_mat)
# M step: Optimization
fn = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
return(-sum(rk*log(Pk) + (fk-rk)*log(1-Pk)))
} # fn funtion : function to be minimized
gr = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
Pk_star = (Pk - par[3])/(1 - par[3]) # c par
Wk =  Pk_star *(1-Pk_star)/(Pk*(1-Pk)) # c par
a.L = -(1 - par[3])*sum((qpoint - par[2])*(rk-fk*Pk)*Wk) # 6.25
b.L = par[1]*(1 - par[3])*sum((rk-fk*Pk)*Wk)             # 6.26
c.L = -1/(1-par[3])*sum((rk-fk*Pk)/Pk)                   # 6.27
return(c(a.L, b.L, c.L))} # gr function: function to return the gradient
par.new = matrix(nrow = I, ncol = 3)
i = 1
for(i in 1:I){
rk = rk_mat[,i] # E-step
res = optim(par.old[i, ], fn, gr, method = "BFGS") # M-step
par.new[i, ] = res$par
}# i loop
if(max(abs(par.old - par.new)) < 0.0001) break
par.old  = par.new
}# repeat
par.new
# True Theta
N = 20000
theta = rnorm(N)
# True Theta
N = 30000
theta = rnorm(N)
ThreePL = function(theta, a, b, c, D = 1){
c + (1-c)/(1 + exp(-D*a*(theta - b)))
}
P = sapply(1:I, function(i){ThreePL(theta, a = TrueA[i], b =TrueB[i], c =  TrueC[i])})
R = matrix(runif(N*I), nrow = N, ncol = I)
resp = ifelse(P > R , 1, 0)
Q = 41
qpoint = seq(from = -4, to = 4, length.out = Q) # X_k 0.2 point로 구분
weight = dnorm(qpoint)/sum(dnorm(qpoint)) # A(X_k) 정규분포 상 해당 집단에 포함될 확률
# 3 PL 초기 4개 문항 Item parameter
par.old = matrix(rep(c(1, 0, 0.2), each = I), nrow= I)
iter = 0
repeat{
iter = iter +1
print(paste0("Iteration: ", iter))
# E step: Update f and r
P_q = sapply(1:I, function(i){
ThreePL(qpoint, a = par.old[i,1], b = par.old[i,2], c =  par.old[i,3])})
table0 = exp(log(P_q)%*%t(resp) + log(1-P_q)%*%t(1-resp)) # 6.21 Q by N
table0 = table0*weight # 6.21 Q by N
demo = colSums(table0) # Q by N, 6.22 = 6.23 without summation over j
f_mat = table0/matrix(rep(demo, each = Q), nrow= Q) # Q by N, 6.22 = 6.23 without summation over j
fk = rowSums(f_mat) # 6.23
rk_mat = f_mat %*% resp # 6.23
# 각  문항에 대해서 맞춘 개 수
#  sum(fk)
#  colSums(rk_mat)
# M step: Optimization
fn = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
return(-sum(rk*log(Pk) + (fk-rk)*log(1-Pk)))
} # fn funtion : function to be minimized
gr = function(par){
Pk = ThreePL(qpoint, a =par[1], b =par[2], c = par[3])
Pk_star = (Pk - par[3])/(1 - par[3]) # c par
Wk =  Pk_star *(1-Pk_star)/(Pk*(1-Pk)) # c par
a.L = -(1 - par[3])*sum((qpoint - par[2])*(rk-fk*Pk)*Wk) # 6.25
b.L = par[1]*(1 - par[3])*sum((rk-fk*Pk)*Wk)             # 6.26
c.L = -1/(1-par[3])*sum((rk-fk*Pk)/Pk)                   # 6.27
return(c(a.L, b.L, c.L))} # gr function: function to return the gradient
par.new = matrix(nrow = I, ncol = 3)
i = 1
for(i in 1:I){
rk = rk_mat[,i] # E-step
res = optim(par.old[i, ], fn, gr, method = "BFGS") # M-step
par.new[i, ] = res$par
}# i loop
if(max(abs(par.old - par.new)) < 0.0001) break
par.old  = par.new
}# repeat
par.new
ts <- ts + P
b <- c(-1.0, 0.75, 0.0, 0.5)
a <- c(0.5, 1.2, 0.8, 0.75)
theta <- seq(-3, 3, .1)
ts <- rep(0, length(theta)) # 빈 리스트를 만드는 것과 동일한 작업
J <- length(b)
for (j in 1:J) {
P <- 1 / (1 + exp(-a[j] * (theta - b[j])))
ts <- ts + P
}
plot(theta, ts, type="l", xlim=c(-3,3), ylim=c(0,J),
xlab="Ability", ylab="True Score")
plot(theta, ts, type="l", xlim=c(-3,3), ylim=c(0,J),
xlab="Ability", ylab="True Score",
main="Test Characteristic Curve")
b <- c(-0.4, -0.3, -0.2, -0.1, 0.0, 0.0, 0.1, 0.2, 0.3, 0.4)
a <- c(1.0, 1.5, 1.2, 1.3, 1.0, 1.6, 1.6, 1.4, 1.1, 1.7)
theta <- seq(-3, 3, 0.1)
J <- length(b)
ii <- matrix(rep(0, length(theta)*J), nrow=length(theta))
i <- rep(0, length(theta))
i <- rep(0, length(theta))
for (j in 1:J) {
P <- 1 / (1 + exp(-a[j] * (theta - b[j])))
ii[,j] <- a[j]**2 * P * (1.0 - P)
i <- i + ii[,j]
}
plot(theta, i, xlim=c(-3,3), ylim=c(0,10), type="l",
xlab="Ability", ylab="Information",
main="Test Information Function")
for (j in 1:J) {
P <- 1 / (1 + exp(-a[j] * (theta - b[j])))
ii[,j] <- a[j]**2 * P * (1.0 - P)
i <- i + ii[,j]
}
theta; ii; i
tif <- function(b, a, c) {
J <- length(b)
if (missing(c)) c <- rep(0, J)
if (missing(a)) a <- rep(1, J)
theta <- seq(-3, 3, 0.1)
ii <- matrix(rep(0, length(theta)*J), nrow=length(theta))
i <- rep(0, length(theta))
for (j in 1:J) {
Pstar <- 1 / (1 + exp(-a[j] * (theta - b[j])))
P <- c[j] + (1 - c[j]) * Pstar
ii[,j] <- a[j]**2 * P * (1.0 - P) * (Pstar / P)**2
i <- i + ii[,j]
}
plot(theta, i, xlim=c(-3,3), ylim=c(0,10), type="l",
xlab="Ability", ylab="Information",
main="Test Information Function")
}
-152.5588 + 285.1182*38 + 30.9018*346
-152.5558 + 285.1182*38 + 30.9018*346
-152.5558 + 285.1882*38 + 30.9018*346
30/140
10.8^2 * 99 - 1.42^2*99
10.8^2 - 1.42^2
11347.74 / 114.6236
10.8^2 * 100 - 10.8^2 * 99 - 10.8^2
setwd('Users/kimjisu/Desktop/seoulbigcam')
setwd('User/kimjisu/Desktop/seoulbigcam')
getwd()
setwd('/Users/kimjisu/Desktop/seoulbigcam')
cl3 = read.table('cluster3.csv')
View(cl3)
cl3 = read.csv('cluster3.csv')
View(cl3)
boxplot(cl3$policy, main="policy", xlab="X 축", ylab="Y 축")
boxplot(cl3$policy, main="policy")
boxplot(cl3$policy, main="live")
boxplot(cl3$live, main="live")
boxplot(cl3$life, main="life")
boxplot(cl3$life, main="wealth")
boxplot(cl3$wealth, main="wealth")
boxplot(cl3$city, main="city")
boxplot(cl3$edu, main="edu")
boxplot(cl3$safe, main="safe")
